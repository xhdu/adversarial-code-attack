## 1. Model

1. **CodeBERT: A Pre-Trained Model for Programming and Natural Languages**. EMNLP 2020. [[pdf](https://arxiv.org/pdf/2002.08155.pdf)] [[code](https://github.com/microsoft/CodeBERT)]
2. **GraphCodeBERT: Pre-training Code Representations with Data Flow**. ICLR 2021. [[pdf](https://arxiv.org/pdf/2009.08366.pdf))] [[code](https://github.com/microsoft/CodeBERT)]
3. **CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation(CodeGPT)**. NeurIPS 2021. [[pdf](https://openreview.net/pdf?id=6lE4dQXaUcb)] [[code](https://github.com/microsoft/CodeXGLUE)]
4. **PLBART: Unified Pre-training for Program Understanding and Generation**. NAACL 2021. [[pdf](https://arxiv.org/pdf/2103.06333.pdf)] [[code](https://github.com/wasiahmad/PLBART)]
5. **CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation**. EMNLP 2021. [[pdf](https://arxiv.org/abs/2109.00859)] [[code](https://github.com/salesforce/CodeT5)]

## 2. Adversarial Attack

1. **Generating Adversarial Examples for Holding Robustness of Source Code Processing Models**, AAAI 2020. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/5469)] [[code](https://github.com/SEKE-Adversary/MHM)]
2. **Adversarial Robustness of Deep Code Comment Generation**, TOSEM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3501256)] [[code](https://github.com/zhangxq-1/ACCENT-repository)]
3. **Natural Attack for Pre-trained Models of Code**, ICSE 2022. [[pdf](https://arxiv.org/pdf/2201.08698.pdf)] [[code](https://github.com/soarsmu/attack-pretrain-models-of-code)]
4. **Towards Robustness of Deep Program Processing Modelsâ€”Detection, Estimation, and Enhancement**, TOSEM 2022. [[pdf](https://dl.acm.org/doi/full/10.1145/3511887)] [[code](https://github.com/SEKE-Adversary/CARROT)]
5. **RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation**, ICSE 2022. [[pdf](https://arxiv.org/abs/2202.06043)] [[code](https://github.com/RoPGen/RoPGen)]

## 3. Reference
1. **An Extensive Study on Pre-trained Models for Program Understanding and Generation**, ISSTA 2022. [[pdf](https://lingming.cs.illinois.edu/publications/issta2022.pdf)] [[code](https://github.com/ZZR0/ISSTA22-CodeStudy)]
